---
title: Process verslag
tags:
---
# Inleiding
Het doel van mijn project was om een toolbox webapplicatie te maken waar mensen gebruik kunnen maken van verschillende use cases van LLMs met context van gevoelige bedrijfsinformatie. 
Het was al vrij snel duidelijk dat ik uiteindelijk mijn eindproduct kon opsplitsen in twee delen: Een webapplicatie die zich gedraagt als een grafische gebruikersinterface waar mensen kunnen communiceren met een LLM op een manier dat erg simpel zou moeten zijn. Daarnaast moest er ook een applicatie deel komen die tekst genereerd vanuit een LLM model. 

# De web applicatie
Gelukkig had ik vanaf het begin van mijn opdracht een goed voorbeeld om te gebruiken voor de werking van de toolbox webapplicatie. Het idee voor mijn uiteindelijk opdracht was ontstaan vanuit het zien van een [soortgelijke website](https://ai.boardofinnovation.com/) waar mijn bedrijf begeleiders kansen zagen voor verbetering en toevoegingen om het specifiek te maken voor de setting binnen mijn stage bedrijf. Doordat ik geen UX designer ben, heb ik erg veel inspiratie van het design van deze website gepakt en het aangepast naar de huisstijl van Handpicked agencies. 

Hiervoor had ik al vrij snel een basis opgezet zodat ik al aan iets kon beginnen zonder dat ik heel diep in een nieuw onderwerp te duiken zoals ik moest doen bij het LLM gedeelte van mijn opdracht. Wat eerder in mijn stageperiode kregen we een kleine uitleg over een techniek die relevant was voor alle stagairs en ook veel gebruikt werd door de medewerkers. Deze techniek was een JavaScript API framework genaamd ExpressJS. Dit werd aanbevolen doordat dit erg simpel werkt en makkelijk in elkaar te zetten is. Uiteindelijk had ik persoonlijk ook wat tips gekregen over de opbouw van de API en dan vooral over de routes en controllers structuur zoals beschreven staat in [[software-architectuur |software architectuur verslag]]. 

Daarna ging er vooral veel tijd zitten in de details laten kloppen aan de UI kan van de applicatie. Het zijn de dingen waar ik als vaak best wat tijd aan kwijt ben doordat ik niet super bekent ben met front-end development. In deze stage heb ik voor het eerst gewerkt met het Angular framework voor de front-end. Dit framework had ik gekozen doordat ik eerder met een ander framework gewerkt heb (React), maar hier niet heel erg tevreden over was. Verder kwam ik tegen dat Angular en ExpressJS erg vaak same gebruikt worden als een stack in web development. Deze wordt de MEAN-stack genoemd en is dus ook waarom ik ervoor gekozen had om MongoDB te gebruiken als database doordat dit ervoor zorgde dat mijn applicatie een bestaande stack gebruikt.

# De LLM
Zelf vond ik het erg interessant om te gaan onderzoeken waar LLMs momenteel tot in staat zijn. Daarom he ik vroeg in mijn process erg veel onderzoek gedaan naar de verschillende open source LLMs die er momenteel bestaan om te kijken wat voor kwaliteit deze leveren. 

Dit was uiteindelijk geen succes, er waren een paar problemen waar ik tegenaan liep. Ten eerste was het de kwaliteit van de modellen. Het was niet makkelijk om modellen te vinden die goed genoeg werkten om echt te gaan testen en dan ook nog iets zinvols en grammaticaal kloppend te genereren. Heel veel taalmodellen staan nog in hun kinderschoenen. Het tweede probleem waar ik tegen aan liep was de fysieke limitaties van mijn hardware, LLMs staan er om bekent dat ze erg veel performance nodig hebben om te kunnen draaien. Hierdoor duurt het erg lang om een stukje tekst te genereren op middelmatige laptop hardware van ongeveer 5 jaar oud. 

Voor de context van mijn opdracht kwam het beter uit dat de LLM teksten in het Nederlands genereerde. Dit was een ander probleem waar ik tegenaan liep. De Nederlandse taal is blijkbaar geen hoge prioriteit voor de data scientists die werken aan LLMs. Er zijn maar weinig LLMs die op niveau Nederlandse teksten kunnen genereren en als ze dit kunnen kloppen deze vaak grammaticaal niet of volgen ze instructies niet goed op. Vaak had ik ook een probleem wat ik zelf voor de grap 'sentient AI' noem. LLM modellen proberen in principe het volgende woord te voorspellen, dit zorgt er in mijn geval soms voor dat sommige LLMs het volgende woord een nieuwe vraag of query vinden.

![[local-llm-example.png]]

Zo vroeg ik hierboven om een kerstmis post te generen op basis van een context. Na een klein verhaaltje te genereren dat taalkundig niet klopt, besloot het model dat het tijd was om de vraag die ik gesteld had aan te passen om nog iets anders te generen. Dit was iets wat ik terug vond bij meerdere modellen, zelfs modellen die vrij hoog aangeschreven staan zoals Meta's Llama-2 model. Om zonder het model aan te passen toch naar een beter eindresultaat van generaties te komen, heb ik veel dingen geprobeerd. Zo ben ik met de parameters gaan spelen en heb ik verschillende manier van instructies geven geprobeerd. Uiteindelijk was ik al best ver door mijn stage periode heen en was ik nog steeds bezig met het vinden van een goed model, toen hebben mijn stagebegeleiders mij geadviseerd om te gaan kijken buiten open source modellen en me ook aangeraden om vooral te gaan kijken naar OpenAI's ChatGPT.

ChatGPT wordt natuurlijk veel gebruikt, maar je wil het liefst geen gevoelige data er naar toe sturen. OpenAI is erg duidelijk over dat ze de chats die je stuurt via de web interface gebruikt worden van het verder trainen van hun model. Hetzelfde geld niet voor de OpenAI API waar je tegen kosten ook gebruik kan maken van OpenAI's verschillende LLMs. Hierdoor is het eindproduct van mijn stage dus een webapplicatie die op de achtergrond ChatGPT gebruikt via OpenAI's API om dingen te kunnen genereren op basis van een use case specifieke context. 

Om dit werkend te krijgen heb ik mijn applicatie een beetje om moeten gooien, maar dit was niet het grootste probleem doordat de package die ik gebruikte op de zelfde manier aansloot bij de OpenAI API als die dat deed met lokaal opgeslagen modellen. De manier waarop ik dit heb aangepakt deed me denken aan de DOT framework research patterns en ook deels aan hoe ik ze in deze context gebruikt heb. 

![[DOT-combi.png]]

De twee methodes die hier het best bij passen zij de choose fitting technology (links) en validate (rechts). Zoals de naam van de eerste al aangeeft in de naam, heb ik deze vooral gebruikt om te kijken hoe het is om met open source LLMs te werken en heb ik uiteindelijk kunnen bepalen dat deze oplossing niet passend is voor mijn context. Een volgende keer zou ik meer stap voor stap gaan werken met het kijken naar verschillende technologieÃ«n in plaats van bijna al mijn tijd stoppen in een technologie op niveau werkend te kunnen krijgen. Veel van de dingen die ik onderzocht heb zijn terug te vinden in mijn [[custom-llms |verslag over custom LLMs]].
